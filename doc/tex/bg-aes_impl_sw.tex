% =============================================================================

In a software-only implementation,
execution of 
both
AES         functionality
and
application functionality (i.e., whatever uses AES)
is 
performed by 
a general-purpose processor core, using features native to and so by default available in the associated base ISA.
Since we only consider use of the RISC-V scalar base ISA, we exclude work on
use of, e.g., vector-like extensions~\cite{Hamburg:09}.

Although not our focus per se, associated techniques are important because, 
e.g., many ISEs can be described as targeting specific feature(s) within a 
baseline software-only implementation.
Work such as that of
Bernstein and Schwabe~\cite{BerSch:08},
Osvik et al.~\cite{OBSC:10},
and
Schwabe and Stoffelen~\cite{SchSto:16}
present and compare a range of techniques across a range of platforms, but,
for completeness, we present a (limited) survey in what follows.

% -----------------------------------------------------------------------------

\paragraph{Compute-oriented.}

A compute-oriented implementation of AES favours
 online     computation, 
thus reducing 
memory footprint
at the cost of increased 
latency.
Following~\cite[Section 4.1]{DaeRij:02}, for example, the idea is to simply
1) adopt an
    array-packed
   representation of state and round key matrices,
   then
2) construct a round implementation by following the algorithmic description
   of each round function in a direct manner.
Addition in $\F_{2^8}$ can be realised using a native XOR instruction; this
native support is seldom afforded to multiplication and inversion, however.
As a result, it is common to pre-compute the \ALG{S-box} and \AESFUNC{xtime} 
functions:
doing so demands pre-computation and storage of a
$
\SI{256}{\byte}
$
look-up table per function, but significantly reduces execution latency.

On platforms where $w = 32$,
Bertoni et al.~\cite{BBFMM:02}
further improve execution latency by exploiting the wider data-path.  Their
idea is to
1) adopt a 
      row-packed
   representation of state and round key matrices,
2) implement
   \AESFUNC{ShiftRows}
   by using native rotation instructions to act on the packed
   rows,
3) implement
   \AESFUNC{MixColumns}
   by harnessing the SIMD Within A Register (SWAR) paradigm:
   by applying
   \AESFUNC{xtime}
   across a packed row in parallel,
   a carefully organised scheme for evaluating
   \AESFUNC{MixColumns}
   can be constructed.

% -----------------------------------------------------------------------------

\paragraph  {Table-oriented.}

A  table-oriented implementation of AES favours
offline pre-computation,
thus reducing 
latency
at the cost of increased 
memory footprint.
The archetypal example of this technique is use of so-called
T-tables~\cite[Section 4.2]{DaeRij:02}.
In short, doing so means
1) adopting a 
   column-packed
   representation of state and round key matrices,
2) pre-computing
   $
   \AESFUNC{MixColumn} \circ \AESFUNC{SubBytes}
   $
   using the tables
   \[
   \begin{array}{cc}
   \begin{array}{lcl}
   T_0[x] &=& \left[\begin{array}{c}
                    \RADIX{02}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{03}{16} \AESMUL \ALG{S-box}( x ) \\
                    \end{array} \right]
   \end{array}
   &
   \begin{array}{lcl}
   T_1[x] &=& \left[\begin{array}{c}
                    \RADIX{03}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{02}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \end{array} \right]
   \end{array}
   \\\\
   \begin{array}{lcl}
   T_2[x] &=& \left[\begin{array}{c}
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{03}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{02}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \end{array} \right]                 
   \end{array}
   &
   \begin{array}{lcl}
   T_3[x] &=& \left[\begin{array}{c}
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{01}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{03}{16} \AESMUL \ALG{S-box}( x ) \\
                    \RADIX{02}{16} \AESMUL \ALG{S-box}( x ) \\
                    \end{array} \right]
   \end{array}
   \end{array}
   \]
   for $x \in \F_{2^8}$,
3) computing each $j$-th column of $\AESRND{s}{r+1}$ as
   \[
   T_0[ \AESRND{s}{r}_{i, j + i \pmod{Nb}} ] \AESADD
   T_1[ \AESRND{s}{r}_{i, j + i \pmod{Nb}} ] \AESADD
   T_2[ \AESRND{s}{r}_{i, j + i \pmod{Nb}} ] \AESADD
   T_3[ \AESRND{s}{r}_{i, j + i \pmod{Nb}} ]
   \]
   where extraction of elements caters for \AESFUNC{ShiftRows}, then XOR'ing 
   the $j$-th column of $\AESRND{rk}{r}$ to cater for \AESFUNC{AddRoundKey}.

As such, each round amounts to a sequence of look-ups into $T_i$, plus XORs 
to combine their result; 
doing so demands pre-computation and storage of a
$
256 \cdot \SI{4}{\byte} = \SI{1}{\kilo\byte}
$
look-up table per $T_i$.
However, note that the overhead related to extraction of each element from 
packed columns representing $\AESRND{s}{r}$ 
(to form look-table offsets) 
is not insignificant:
Fiskiran and Lee~\cite{FisLee:01}
analyse the impact of different addressing modes on this issue, with
Stoffelen~\cite[Section 3.1]{Stoffelen:19}
concluding that RISC-V is (relatively) ill-equipped to reduce said overhead,
due to the provision of a (relatively) sparse set of addressing modes.

% -----------------------------------------------------------------------------

\paragraph{Bit-sliced.}

The term bit-slicing refers to an implementation technique due to
Biham~\cite{Biham:97},
which constitutes

\begin{enumerate}
\item a non-standard {\em representation}
      of data:
      each $w$-bit word $x$ is transformed into $\REP{x}$,
      i.e.,
      $w$ slices, say $\REP{x}[ i ]$ for
      $
      0 \leq i < w ,
      $
      where $\REP{x}[ i ]_j = x_i$ for some $j$,
      and
\item a non-standard {\em implementation}
      of operation:
      each operation $f$ used as
      $
          {r} =     {f}(     {x} )
      $
      must be transformed into ``software circuit'' $\REP{f}$,
      i.e.,
      a sequence of Boolean instructions acting on the slices st.
      $
      \REP{r} = \REP{f}( \REP{x} ) .
      $
\end{enumerate}

\noindent
Using it introduces some overhead related to conversion of $x$ into
$\REP{x}$ and $\REP{r}$ into $r$, plus the (relative) inefficiency
of $\REP{f}$ vs. $f$ (wrt. latency and footprint).
Crucially, however, if each slice is itself a $w$-bit word, then it
is possible to compute $w$ instances of $\REP{f}$ in {\em parallel}
on suitably packed $\REP{x}$.
A common analogy is that of bit-slicing transforming the 
$w$-bit, $1$-way scalar processor 
into a 
$1$-bit, $w$-way SIMD   processor, 
thus yielding (or recouping) upto a $w$-fold improvement in latency.

As evidenced by various work
(see, e.g., \cite{MatNak:07,Konighofer:08,KasSch:09}),
the application of bit-slicing to AES can be very effective;
Stoffelen~\cite[Section 3.1]{Stoffelen:19}
specifically investigates this fact within the context of RISC-V.

% =============================================================================
